{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA1\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Author: Pan Yang (panyangnlp@gmail.com)\n",
    "# Copyright 2017 @ Yu Zhen\n",
    "import gensim\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from time import time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleanrn = re.compile('\\n')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    cleantext = re.sub(cleanrn,'',cleantext)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk(self.dirname):\n",
    "            for filename in files:\n",
    "                file_path = (root + '/' + filename)\n",
    "                for line in open(file_path,encoding='utf-8'):\n",
    "                    sline = line.strip()\n",
    "                    if sline == \"\":\n",
    "                        continue\n",
    "                    rline = cleanhtml(sline)\n",
    "                    tokenized_line = ' '.join(word_tokenize(rline))\n",
    "                    is_alpha_word_line = [word for word in\n",
    "                                          tokenized_line.lower().split()\n",
    "                                          if word.isalpha()]\n",
    "                    yield is_alpha_word_line\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-15 16:51:39,375 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-07-15 16:51:45,541 : INFO : adding document #10000 to Dictionary(16330 unique tokens: ['terrorists', 'are', 'now', 'being', 'draped']...)\n",
      "2017-07-15 16:51:52,030 : INFO : adding document #20000 to Dictionary(24572 unique tokens: ['terrorists', 'are', 'now', 'being', 'draped']...)\n",
      "2017-07-15 16:51:58,571 : INFO : adding document #30000 to Dictionary(31179 unique tokens: ['terrorists', 'are', 'now', 'being', 'draped']...)\n",
      "2017-07-15 16:51:59,268 : INFO : built Dictionary(31717 unique tokens: ['terrorists', 'are', 'now', 'being', 'draped']...) from 31013 documents (total 423684 corpus positions)\n",
      "2017-07-15 16:52:18,618 : INFO : collecting all words and their counts\n",
      "2017-07-15 16:52:18,625 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-07-15 16:52:24,196 : INFO : PROGRESS: at sentence #10000, processed 139421 words, keeping 16330 word types\n",
      "2017-07-15 16:52:30,224 : INFO : PROGRESS: at sentence #20000, processed 273612 words, keeping 24572 word types\n",
      "2017-07-15 16:52:36,196 : INFO : PROGRESS: at sentence #30000, processed 409006 words, keeping 31179 word types\n",
      "2017-07-15 16:52:36,826 : INFO : collected 31717 word types from a corpus of 423684 raw words and 31013 sentences\n",
      "2017-07-15 16:52:36,831 : INFO : Loading a fresh vocabulary\n",
      "2017-07-15 16:52:36,871 : INFO : min_count=5 retains 6745 unique words (21% of original 31717, drops 24972)\n",
      "2017-07-15 16:52:36,875 : INFO : min_count=5 leaves 386947 word corpus (91% of original 423684, drops 36737)\n",
      "2017-07-15 16:52:36,919 : INFO : deleting the raw counts dictionary of 31717 items\n",
      "2017-07-15 16:52:36,925 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2017-07-15 16:52:36,929 : INFO : downsampling leaves estimated 287329 word corpus (74.3% of prior 386947)\n",
      "2017-07-15 16:52:36,936 : INFO : estimated required memory for 6745 words and 100 dimensions: 8768500 bytes\n",
      "2017-07-15 16:52:36,976 : INFO : resetting layer weights\n",
      "2017-07-15 16:52:37,130 : INFO : training model with 8 workers on 6745 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-07-15 16:52:38,269 : INFO : PROGRESS: at 1.39% examples, 18020 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:39,572 : INFO : PROGRESS: at 2.81% examples, 16721 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:40,761 : INFO : PROGRESS: at 4.18% examples, 16753 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:41,923 : INFO : PROGRESS: at 5.54% examples, 16893 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:43,130 : INFO : PROGRESS: at 6.93% examples, 16864 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:44,380 : INFO : PROGRESS: at 8.32% examples, 16751 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:45,462 : INFO : PROGRESS: at 9.35% examples, 16196 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:46,829 : INFO : PROGRESS: at 10.83% examples, 16000 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:48,069 : INFO : PROGRESS: at 12.25% examples, 16047 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:49,312 : INFO : PROGRESS: at 13.66% examples, 16074 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:50,621 : INFO : PROGRESS: at 15.10% examples, 16021 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:51,959 : INFO : PROGRESS: at 16.54% examples, 15947 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:53,321 : INFO : PROGRESS: at 17.97% examples, 15873 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:54,632 : INFO : PROGRESS: at 19.38% examples, 15862 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:55,823 : INFO : PROGRESS: at 20.73% examples, 15958 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:57,093 : INFO : PROGRESS: at 22.16% examples, 15957 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:58,357 : INFO : PROGRESS: at 23.55% examples, 15952 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:52:59,590 : INFO : PROGRESS: at 24.91% examples, 15975 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:00,792 : INFO : PROGRESS: at 26.28% examples, 16018 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:02,047 : INFO : PROGRESS: at 27.67% examples, 16020 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:03,500 : INFO : PROGRESS: at 29.14% examples, 15909 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:04,516 : INFO : PROGRESS: at 30.17% examples, 15810 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:05,779 : INFO : PROGRESS: at 31.61% examples, 15823 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:07,017 : INFO : PROGRESS: at 33.00% examples, 15849 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:08,288 : INFO : PROGRESS: at 34.42% examples, 15852 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:09,637 : INFO : PROGRESS: at 35.86% examples, 15825 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:10,961 : INFO : PROGRESS: at 37.30% examples, 15808 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:12,354 : INFO : PROGRESS: at 38.73% examples, 15769 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:13,650 : INFO : PROGRESS: at 40.09% examples, 15778 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:14,796 : INFO : PROGRESS: at 41.49% examples, 15836 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:16,106 : INFO : PROGRESS: at 42.90% examples, 15824 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:17,302 : INFO : PROGRESS: at 44.27% examples, 15854 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:18,483 : INFO : PROGRESS: at 45.64% examples, 15891 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:19,715 : INFO : PROGRESS: at 47.03% examples, 15907 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:20,986 : INFO : PROGRESS: at 48.42% examples, 15908 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:22,081 : INFO : PROGRESS: at 49.45% examples, 15819 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:23,455 : INFO : PROGRESS: at 50.94% examples, 15789 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:24,673 : INFO : PROGRESS: at 52.35% examples, 15815 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:25,935 : INFO : PROGRESS: at 53.76% examples, 15821 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:27,255 : INFO : PROGRESS: at 55.20% examples, 15810 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:28,602 : INFO : PROGRESS: at 56.64% examples, 15792 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:29,932 : INFO : PROGRESS: at 58.07% examples, 15782 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:31,221 : INFO : PROGRESS: at 59.47% examples, 15788 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:32,435 : INFO : PROGRESS: at 60.82% examples, 15813 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:33,710 : INFO : PROGRESS: at 62.26% examples, 15814 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:34,966 : INFO : PROGRESS: at 63.64% examples, 15819 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:36,149 : INFO : PROGRESS: at 65.00% examples, 15844 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:37,350 : INFO : PROGRESS: at 66.38% examples, 15865 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:38,539 : INFO : PROGRESS: at 67.77% examples, 15886 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:40,001 : INFO : PROGRESS: at 69.25% examples, 15839 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:41,028 : INFO : PROGRESS: at 70.26% examples, 15796 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:42,290 : INFO : PROGRESS: at 71.71% examples, 15803 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:43,558 : INFO : PROGRESS: at 73.10% examples, 15807 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:44,863 : INFO : PROGRESS: at 74.52% examples, 15802 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:46,188 : INFO : PROGRESS: at 75.97% examples, 15794 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:47,500 : INFO : PROGRESS: at 77.41% examples, 15790 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:48,828 : INFO : PROGRESS: at 78.83% examples, 15785 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:50,145 : INFO : PROGRESS: at 80.18% examples, 15783 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:51,222 : INFO : PROGRESS: at 81.59% examples, 15827 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:52,527 : INFO : PROGRESS: at 83.00% examples, 15821 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-15 16:53:53,756 : INFO : PROGRESS: at 84.37% examples, 15830 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:54,903 : INFO : PROGRESS: at 85.73% examples, 15855 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:56,146 : INFO : PROGRESS: at 87.12% examples, 15861 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:57,503 : INFO : PROGRESS: at 88.53% examples, 15847 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:58,593 : INFO : PROGRESS: at 89.57% examples, 15800 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:53:59,930 : INFO : PROGRESS: at 91.05% examples, 15790 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:01,134 : INFO : PROGRESS: at 92.45% examples, 15806 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:02,379 : INFO : PROGRESS: at 93.86% examples, 15813 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:03,696 : INFO : PROGRESS: at 95.29% examples, 15807 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:05,006 : INFO : PROGRESS: at 96.74% examples, 15803 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:06,349 : INFO : PROGRESS: at 98.17% examples, 15795 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:07,667 : INFO : PROGRESS: at 99.56% examples, 15794 words/s, in_qsize 0, out_qsize 0\n",
      "2017-07-15 16:54:08,052 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-07-15 16:54:08,059 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-15 16:54:08,061 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-15 16:54:08,063 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-15 16:54:08,067 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-15 16:54:08,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-15 16:54:08,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-15 16:54:08,084 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-15 16:54:08,087 : INFO : training on 2118420 raw words (1436678 effective words) took 90.9s, 15796 effective words/s\n",
      "2017-07-15 16:54:08,092 : INFO : saving Word2Vec object under data/model/word2vec_gensim, separately None\n",
      "2017-07-15 16:54:08,096 : INFO : not storing attribute syn0norm\n",
      "2017-07-15 16:54:08,102 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/model/word2vec_gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m             \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved %s object\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1f914b473f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                                \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                workers=multiprocessing.cpu_count())\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/model/word2vec_gensim\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m model.wv.save_word2vec_format(\"data/model/word2vec_org\",\n\u001b[0;32m     14\u001b[0m                               \u001b[1;34m\"data/model/vocabulary\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'syn0norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cum_table'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             self._smart_save(fname_or_handle, separately, sep_limit, ignore,\n\u001b[1;32m--> 499\u001b[1;33m                              pickle_protocol=pickle_protocol)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;31m#endclass SaveLoad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    369\u001b[0m                                        compress, subname)\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[1;31m# restore attribs handled specially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mpickle\u001b[1;34m(obj, fname, protocol)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \"\"\"\n\u001b[1;32m--> 924\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3u\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA1\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \"\"\"\n\u001b[1;32m--> 644\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/model/word2vec_gensim'"
     ]
    }
   ],
   "source": [
    "data_path = 'C:/Users/Administrator/Desktop/test'\n",
    "begin = time()\n",
    "\n",
    "sentences = MySentences(data_path)\n",
    "dictionary = gensim.corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "model = gensim.models.Word2Vec(sentences,\n",
    "                               size=100,\n",
    "                               window=5,\n",
    "                               min_count=5,\n",
    "                               workers=multiprocessing.cpu_count())\n",
    "model.save(\"data/model/word2vec_gensim\")\n",
    "model.wv.save_word2vec_format(\"data/model/word2vec_org\",\n",
    "                              \"data/model/vocabulary\",\n",
    "                              binary=False)\n",
    "\n",
    "end = time()\n",
    "print\n",
    "\"Total procesing time: %d seconds\" % (end - begin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-2-1161036a9e3a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-1161036a9e3a>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    tfidf_model = gensim.models.TfidfModel(corpus)\u001b[0m\n\u001b[1;37m                                                  ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "tfidf_m = tfidf_model[corpus]\n",
    "lda = gensim.models.LdaModel(tfidf_m, id2word=dictionary, num_topics=200)\n",
    "corpus_lda = lda[tfidf_m]\n",
    "lda_csc_matrix = gensim.matutils.corpus2csc(corpus_lda).transpose()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmean = KMeans(n_clusters=10)\n",
    "kmean.fit(lda_csc_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from sklearn.decomposition import PCA\n",
    "\n",
    "    weight = lda_csc_matrix.toArray()\n",
    "    pca = PCA(n_components=2)  # 输出两维\n",
    "    newData = pca.fit_transform(weight)  # 载入N维\n",
    "    print(newData)\n",
    "\n",
    "    # 5A景区\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    i = 0\n",
    "    while i < 400:\n",
    "        x1.append(newData[i][0])\n",
    "        y1.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 动物\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    i = 400\n",
    "    while i < 600:\n",
    "        x2.append(newData[i][0])\n",
    "        y2.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 人物\n",
    "    x3 = []\n",
    "    y3 = []\n",
    "    i = 600\n",
    "    while i < 800:\n",
    "        x3.append(newData[i][0])\n",
    "        y3.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 国家\n",
    "    x4 = []\n",
    "    y4 = []\n",
    "    i = 800\n",
    "    while i < 1000:\n",
    "        x4.append(newData[i][0])\n",
    "        y4.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 四种颜色 红 绿 蓝 黑\n",
    "    PCA.plt.plot(x1, y1, 'or')\n",
    "    PCA.plt.plot(x2, y2, 'og')\n",
    "    PCA.plt.plot(x3, y3, 'ob')\n",
    "    PCA.plt.plot(x4, y4, 'ok')\n",
    "    PCA.plt.show()\n",
    "\n",
    "    data_path = sys.argv[1]\n",
    "    begin = time()\n",
    "\n",
    "    sentences = MySentences(data_path)\n",
    "    dictionary = gensim.corpora.Dictionary(sentences)\n",
    "    corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "    model = gensim.models.Word2Vec(sentences,\n",
    "                                   size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=5,\n",
    "                                   workers=multiprocessing.cpu_count())\n",
    "\n",
    "    model.save(\"data/model/word2vec_gensim\")\n",
    "    model.wv.save_word2vec_format(\"data/model/word2vec_org\",\n",
    "                                  \"data/model/vocabulary\",\n",
    "                                  binary=False)\n",
    "\n",
    "    end = time()\n",
    "    print\n",
    "    \"Total procesing time: %d seconds\" % (end - begin)\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
